{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5269caa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~treamlit (E:\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~treamlit (E:\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~treamlit (e:\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -q transformers==4.36.0 peft==0.7.1 datasets==2.16.0 accelerate==0.25.0 bitsandbytes==0.41.3 wandb scikit-learn\n",
    "\n",
    "import os\n",
    "os.environ['WANDB_DISABLED'] = 'true'  # T·∫Øt n·∫øu kh√¥ng d√πng wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7b6407e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "E:\\Lib\\site-packages\\transformers\\utils\\generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.training_args because of the following error (look up to see its traceback):\nmodule 'torch' has no attribute 'version'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1382\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   1381\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1382\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1383\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Lib\\importlib\\__init__.py:90\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     89\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1387\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1331\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:935\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:995\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:488\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Lib\\site-packages\\transformers\\training_args.py:69\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_accelerate_available():\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01maccelerate\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstate\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AcceleratorState, PartialState\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01maccelerate\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DistributedType\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Lib\\site-packages\\accelerate\\__init__.py:3\u001b[39m\n\u001b[32m      1\u001b[39m __version__ = \u001b[33m\"\u001b[39m\u001b[33m0.25.0\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01maccelerator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Accelerator\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbig_modeling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      5\u001b[39m     cpu_offload,\n\u001b[32m      6\u001b[39m     cpu_offload_with_hook,\n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m     load_checkpoint_and_dispatch,\n\u001b[32m     12\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Lib\\site-packages\\accelerate\\accelerator.py:35\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhooks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhooks\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcheckpointing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_accelerator_state, load_custom_state, save_accelerator_state, save_custom_state\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoaderDispatcher, prepare_data_loader, skip_first_batches\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Lib\\site-packages\\accelerate\\checkpointing.py:24\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcuda\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mamp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GradScaler\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     25\u001b[39m     MODEL_NAME,\n\u001b[32m     26\u001b[39m     OPTIMIZER_NAME,\n\u001b[32m     27\u001b[39m     RNG_STATE_NAME,\n\u001b[32m     28\u001b[39m     SAFE_MODEL_NAME,\n\u001b[32m     29\u001b[39m     SAFE_WEIGHTS_NAME,\n\u001b[32m     30\u001b[39m     SAMPLER_NAME,\n\u001b[32m     31\u001b[39m     SCALER_NAME,\n\u001b[32m     32\u001b[39m     SCHEDULER_NAME,\n\u001b[32m     33\u001b[39m     WEIGHTS_NAME,\n\u001b[32m     34\u001b[39m     get_pretty_name,\n\u001b[32m     35\u001b[39m     is_tpu_available,\n\u001b[32m     36\u001b[39m     is_xpu_available,\n\u001b[32m     37\u001b[39m     save,\n\u001b[32m     38\u001b[39m )\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tpu_available(check_device=\u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Lib\\site-packages\\accelerate\\utils\\__init__.py:79\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimports\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     49\u001b[39m     get_ccl_version,\n\u001b[32m     50\u001b[39m     is_4bit_bnb_available,\n\u001b[32m   (...)\u001b[39m\u001b[32m     77\u001b[39m     is_xpu_available,\n\u001b[32m     78\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     80\u001b[39m     calculate_maximum_sizes,\n\u001b[32m     81\u001b[39m     check_device_map,\n\u001b[32m     82\u001b[39m     check_tied_parameters_in_config,\n\u001b[32m     83\u001b[39m     check_tied_parameters_on_same_device,\n\u001b[32m     84\u001b[39m     compute_module_sizes,\n\u001b[32m     85\u001b[39m     convert_file_size_to_int,\n\u001b[32m     86\u001b[39m     dtype_byte_size,\n\u001b[32m     87\u001b[39m     find_tied_parameters,\n\u001b[32m     88\u001b[39m     get_balanced_memory,\n\u001b[32m     89\u001b[39m     get_max_layer_size,\n\u001b[32m     90\u001b[39m     get_max_memory,\n\u001b[32m     91\u001b[39m     get_mixed_precision_context_manager,\n\u001b[32m     92\u001b[39m     id_tensor_storage,\n\u001b[32m     93\u001b[39m     infer_auto_device_map,\n\u001b[32m     94\u001b[39m     load_checkpoint_in_model,\n\u001b[32m     95\u001b[39m     load_offloaded_weights,\n\u001b[32m     96\u001b[39m     load_state_dict,\n\u001b[32m     97\u001b[39m     named_module_tensors,\n\u001b[32m     98\u001b[39m     retie_parameters,\n\u001b[32m     99\u001b[39m     set_module_tensor_to_device,\n\u001b[32m    100\u001b[39m     shard_checkpoint,\n\u001b[32m    101\u001b[39m )\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01moffload\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    103\u001b[39m     OffloadedWeightsLoader,\n\u001b[32m    104\u001b[39m     PrefixedDataset,\n\u001b[32m   (...)\u001b[39m\u001b[32m    109\u001b[39m     save_offload_index,\n\u001b[32m    110\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Lib\\site-packages\\accelerate\\utils\\modeling.py:30\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstate\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AcceleratorState\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SAFE_WEIGHTS_NAME, WEIGHTS_NAME\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Lib\\site-packages\\accelerate\\state.py:48\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataclasses\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SageMakerDistributedType\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_tpu_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheck_device\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch_xla\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mxla_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxm\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Lib\\site-packages\\accelerate\\utils\\imports.py:96\u001b[39m, in \u001b[36mis_tpu_available\u001b[39m\u001b[34m(check_device)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# Due to bugs on the amp series GPUs, we disable torch-xla on them\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_cuda_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     97\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Lib\\site-packages\\accelerate\\utils\\imports.py:86\u001b[39m, in \u001b[36mis_cuda_available\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     85\u001b[39m     os.environ[\u001b[33m\"\u001b[39m\u001b[33mPYTORCH_NVML_BASED_CUDA_CHECK\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mstr\u001b[39m(\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     available = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Lib\\site-packages\\torch\\cuda\\__init__.py:169\u001b[39m, in \u001b[36mis_available\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _nvml_based_avail():\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# The user has set an env variable to request this availability check that attempts to avoid fork poisoning by\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;66;03m# using NVML at the cost of a weaker CUDA availability assessment. Note that if NVML discovery/initialization\u001b[39;00m\n\u001b[32m    168\u001b[39m     \u001b[38;5;66;03m# fails, this assessment falls back to the default CUDA Runtime API assessment (`cudaGetDeviceCount`)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdevice_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m > \u001b[32m0\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    171\u001b[39m     \u001b[38;5;66;03m# The default availability inspection never throws and returns 0 if the driver is missing or can't\u001b[39;00m\n\u001b[32m    172\u001b[39m     \u001b[38;5;66;03m# be initialized. This uses the CUDA Runtime API `cudaGetDeviceCount` which in turn initializes the CUDA Driver\u001b[39;00m\n\u001b[32m    173\u001b[39m     \u001b[38;5;66;03m# API via `cuInit`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Lib\\site-packages\\torch\\cuda\\__init__.py:990\u001b[39m, in \u001b[36mdevice_count\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    989\u001b[39m \u001b[38;5;66;03m# bypass _device_count_nvml() if rocm (not supported)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m990\u001b[39m nvml_count = _device_count_amdsmi() \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mversion\u001b[49m.hip \u001b[38;5;28;01melse\u001b[39;00m _device_count_nvml()\n\u001b[32m    991\u001b[39m r = torch._C._cuda_getDeviceCount() \u001b[38;5;28;01mif\u001b[39;00m nvml_count < \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m nvml_count\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Lib\\site-packages\\torch\\__init__.py:2688\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m   2686\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[34m__name__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2688\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmodule \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'torch' has no attribute 'version'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      6\u001b[39m     AutoModelForCausalLM, \n\u001b[32m      7\u001b[39m     AutoTokenizer,\n\u001b[32m      8\u001b[39m     TrainingArguments,\n\u001b[32m      9\u001b[39m     Trainer,\n\u001b[32m     10\u001b[39m     BitsAndBytesConfig\n\u001b[32m     11\u001b[39m )\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     13\u001b[39m     LoraConfig,\n\u001b[32m     14\u001b[39m     get_peft_model,\n\u001b[32m     15\u001b[39m     prepare_model_for_kbit_training,\n\u001b[32m     16\u001b[39m     TaskType\n\u001b[32m     17\u001b[39m )\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1412\u001b[39m, in \u001b[36m_handle_fromlist\u001b[39m\u001b[34m(module, fromlist, import_, recursive)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1372\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1370\u001b[39m     value = \u001b[38;5;28mself\u001b[39m._get_module(name)\n\u001b[32m   1371\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module.keys():\n\u001b[32m-> \u001b[39m\u001b[32m1372\u001b[39m     module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1373\u001b[39m     value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m   1374\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1384\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   1382\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m + module_name, \u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m)\n\u001b[32m   1383\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1384\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1385\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m because of the following error (look up to see its\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1386\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1387\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Failed to import transformers.training_args because of the following error (look up to see its traceback):\nmodule 'torch' has no attribute 'version'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    TaskType\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional\n",
    "import gc\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "\n",
    "# Config\n",
    "class Config:\n",
    "    # Models\n",
    "    TEACHER_MODEL = \"meta-llama/Llama-2-13b-hf\"  # Ho·∫∑c \"NousResearch/Llama-2-13b-hf\"\n",
    "    STUDENT_MODEL = \"mistralai/Mistral-7B-v0.1\"\n",
    "    \n",
    "    # Dataset\n",
    "    DATASET_NAME = \"gsm8k\"\n",
    "    DATASET_CONFIG = \"main\"\n",
    "    MAX_SAMPLES = 2000  # Gi·ªõi h·∫°n cho Kaggle\n",
    "    MAX_LENGTH = 512\n",
    "    \n",
    "    # Training\n",
    "    BATCH_SIZE = 2\n",
    "    GRADIENT_ACCUM = 8\n",
    "    LEARNING_RATE = 2e-4\n",
    "    NUM_EPOCHS = 3\n",
    "    WARMUP_STEPS = 100\n",
    "    \n",
    "    # Distillation\n",
    "    ALPHA_OUTPUT = 0.5  # Output loss weight\n",
    "    BETA_LATENT = 0.5   # Latent loss weight\n",
    "    TEMPERATURE = 2.0\n",
    "    LATENT_LAYERS = [8, 16, 24]  # Layers to match\n",
    "    \n",
    "    # LoRA\n",
    "    LORA_R = 16\n",
    "    LORA_ALPHA = 32\n",
    "    LORA_DROPOUT = 0.05\n",
    "    LORA_TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "    \n",
    "    # Paths\n",
    "    OUTPUT_DIR = \"/kaggle/working/distill_output\"\n",
    "    LATENT_CACHE_DIR = \"/kaggle/working/latent_cache\"\n",
    "    \n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "config = Config()\n",
    "os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(config.LATENT_CACHE_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"üî• Device: {config.DEVICE}\")\n",
    "print(f\"üî• GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1881a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prompt(question: str, answer: str = None) -> str:\n",
    "    \"\"\"Format prompt for reasoning task\"\"\"\n",
    "    prompt = f\"Question: {question}\\n\\nLet's solve this step by step:\\n\"\n",
    "    \"\"\"This really need to be improved later\"\"\"\n",
    "    if answer:\n",
    "        prompt += f\"{answer}\"\n",
    "    return prompt\n",
    "\n",
    "class ReasoningDataset(Dataset):\n",
    "    \"\"\"Custom dataset with latent cache support\"\"\"\n",
    "    def __init__(self, data, tokenizer, max_length=512, latent_dir=None):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.latent_dir = latent_dir\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Tokenize\n",
    "        prompt = prepare_prompt(item['question'], item.get('answer'))\n",
    "        encoding = self.tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        result = {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'idx': idx\n",
    "        }\n",
    "        \n",
    "        # Load cached latent if available\n",
    "        if self.latent_dir:\n",
    "            latent_path = os.path.join(self.latent_dir, f\"latent_{idx}.pt\")\n",
    "            if os.path.exists(latent_path):\n",
    "                result['teacher_latents'] = torch.load(latent_path)\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Load GSM8K dataset\n",
    "print(\"üì¶ Loading GSM8K dataset...\")\n",
    "dataset = load_dataset(config.DATASET_NAME, config.DATASET_CONFIG)\n",
    "\n",
    "# Sample subset for Kaggle\n",
    "train_data = dataset['train'].select(range(min(config.MAX_SAMPLES, len(dataset['train']))))\n",
    "test_data = dataset['test'].select(range(min(500, len(dataset['test']))))\n",
    "\n",
    "print(f\"‚úÖ Train: {len(train_data)} | Test: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbc4719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_teacher_model():\n",
    "    \"\"\"Load teacher with 4-bit quantization to save memory\"\"\"\n",
    "    print(\"üîÑ Loading Teacher Model (4-bit)...\")\n",
    "    \n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        config.TEACHER_MODEL,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.TEACHER_MODEL)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def extract_latent_states(model, tokenizer, data, output_dir, batch_size=1):\n",
    "    \"\"\"Extract and cache teacher's latent states\"\"\"\n",
    "    print(f\"üß† Extracting latent states to {output_dir}...\")\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx in tqdm(range(len(data)), desc=\"Extracting\"):\n",
    "            cache_path = os.path.join(output_dir, f\"latent_{idx}.pt\")\n",
    "            \n",
    "            # Skip if already cached\n",
    "            if os.path.exists(cache_path):\n",
    "                continue\n",
    "            \n",
    "            item = data[idx]\n",
    "            prompt = prepare_prompt(item['question'], item.get('answer'))\n",
    "            \n",
    "            inputs = tokenizer(\n",
    "                prompt,\n",
    "                return_tensors='pt',\n",
    "                truncation=True,\n",
    "                max_length=config.MAX_LENGTH\n",
    "            ).to(model.device)\n",
    "            \n",
    "            # Forward pass with hidden states\n",
    "            outputs = model(\n",
    "                **inputs,\n",
    "                output_hidden_states=True,\n",
    "                return_dict=True\n",
    "            )\n",
    "            \n",
    "            # Extract specific layers\n",
    "            latent_states = {}\n",
    "            for layer_idx in config.LATENT_LAYERS:\n",
    "                if layer_idx < len(outputs.hidden_states):\n",
    "                    # Average pool over sequence\n",
    "                    hidden = outputs.hidden_states[layer_idx]\n",
    "                    pooled = hidden.mean(dim=1).cpu()  # [batch, hidden_dim]\n",
    "                    latent_states[f'layer_{layer_idx}'] = pooled\n",
    "            \n",
    "            # Save\n",
    "            torch.save(latent_states, cache_path)\n",
    "            \n",
    "            # Free memory\n",
    "            del outputs, inputs\n",
    "            if idx % 100 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"‚úÖ Latent extraction complete!\")\n",
    "\n",
    "# Extract latents (comment out if already done)\n",
    "EXTRACT_LATENTS = True  # Set False if cache exists\n",
    "\n",
    "if EXTRACT_LATENTS:\n",
    "    teacher_model, teacher_tokenizer = load_teacher_model()\n",
    "    extract_latent_states(\n",
    "        teacher_model, \n",
    "        teacher_tokenizer, \n",
    "        train_data, \n",
    "        config.LATENT_CACHE_DIR\n",
    "    )\n",
    "    \n",
    "    # Free teacher model\n",
    "    del teacher_model, teacher_tokenizer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"üóëÔ∏è  Teacher model freed from memory\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b02616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_student_model():\n",
    "    \"\"\"Load student model with LoRA\"\"\"\n",
    "    print(\"üéì Loading Student Model with LoRA...\")\n",
    "    \n",
    "    # Load base model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        config.STUDENT_MODEL,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.STUDENT_MODEL)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # LoRA config\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        r=config.LORA_R,\n",
    "        lora_alpha=config.LORA_ALPHA,\n",
    "        lora_dropout=config.LORA_DROPOUT,\n",
    "        target_modules=config.LORA_TARGET_MODULES,\n",
    "        bias=\"none\"\n",
    "    )\n",
    "    \n",
    "    # Apply LoRA\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "student_model, student_tokenizer = setup_student_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00c7339",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillationTrainer(Trainer):\n",
    "    \"\"\"Custom trainer with latent distillation loss\"\"\"\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # Get student outputs with hidden states\n",
    "        outputs = model(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            labels=inputs['input_ids'],\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        # 1. Output loss (standard language modeling)\n",
    "        loss_output = outputs.loss\n",
    "        \n",
    "        # 2. Latent distillation loss\n",
    "        loss_latent = 0.0\n",
    "        if 'teacher_latents' in inputs:\n",
    "            teacher_latents = inputs['teacher_latents']\n",
    "            student_hidden = outputs.hidden_states\n",
    "            \n",
    "            num_latent_layers = 0\n",
    "            for layer_idx in config.LATENT_LAYERS:\n",
    "                layer_key = f'layer_{layer_idx}'\n",
    "                if layer_key in teacher_latents and layer_idx < len(student_hidden):\n",
    "                    # Get student hidden at same layer\n",
    "                    student_h = student_hidden[layer_idx]\n",
    "                    student_pooled = student_h.mean(dim=1)  # [batch, hidden]\n",
    "                    \n",
    "                    # Teacher latent\n",
    "                    teacher_h = teacher_latents[layer_key].to(student_pooled.device)\n",
    "                    \n",
    "                    # MSE loss\n",
    "                    loss_latent += F.mse_loss(student_pooled, teacher_h)\n",
    "                    num_latent_layers += 1\n",
    "            \n",
    "            if num_latent_layers > 0:\n",
    "                loss_latent /= num_latent_layers\n",
    "        \n",
    "        # Combined loss\n",
    "        total_loss = (config.ALPHA_OUTPUT * loss_output + \n",
    "                      config.BETA_LATENT * loss_latent)\n",
    "        \n",
    "        return (total_loss, outputs) if return_outputs else total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e9e886",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ReasoningDataset(\n",
    "    train_data,\n",
    "    student_tokenizer,\n",
    "    max_length=config.MAX_LENGTH,\n",
    "    latent_dir=config.LATENT_CACHE_DIR\n",
    ")\n",
    "\n",
    "test_dataset = ReasoningDataset(\n",
    "    test_data,\n",
    "    student_tokenizer,\n",
    "    max_length=config.MAX_LENGTH,\n",
    "    latent_dir=None  # No latent for test\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=config.OUTPUT_DIR,\n",
    "    num_train_epochs=config.NUM_EPOCHS,\n",
    "    per_device_train_batch_size=config.BATCH_SIZE,\n",
    "    per_device_eval_batch_size=config.BATCH_SIZE,\n",
    "    gradient_accumulation_steps=config.GRADIENT_ACCUM,\n",
    "    learning_rate=config.LEARNING_RATE,\n",
    "    warmup_steps=config.WARMUP_STEPS,\n",
    "    logging_steps=50,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = DistillationTrainer(\n",
    "    model=student_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "print(\"üöÄ Training configuration ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655bad9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üî• Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save final model\n",
    "trainer.save_model(f\"{config.OUTPUT_DIR}/final_model\")\n",
    "student_tokenizer.save_pretrained(f\"{config.OUTPUT_DIR}/final_model\")\n",
    "\n",
    "print(\"‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276e9e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_reasoning(model, tokenizer, test_data, num_samples=50):\n",
    "    \"\"\"Evaluate reasoning accuracy\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    print(\"üìä Evaluating reasoning accuracy...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx in tqdm(range(min(num_samples, len(test_data)))):\n",
    "            item = test_data[idx]\n",
    "            prompt = prepare_prompt(item['question'])\n",
    "            \n",
    "            inputs = tokenizer(\n",
    "                prompt,\n",
    "                return_tensors='pt',\n",
    "                truncation=True,\n",
    "                max_length=256\n",
    "            ).to(model.device)\n",
    "            \n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=128,\n",
    "                temperature=0.7,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            \n",
    "            generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Simple accuracy check (contains answer)\n",
    "            ground_truth = str(item['answer'])\n",
    "            if ground_truth in generated:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "            \n",
    "            # Print first 3 examples\n",
    "            if idx < 3:\n",
    "                print(f\"\\n{'='*60}\")\n",
    "                print(f\"Q: {item['question']}\")\n",
    "                print(f\"Ground Truth: {ground_truth}\")\n",
    "                print(f\"Generated: {generated[len(prompt):][:200]}...\")\n",
    "    \n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    print(f\"\\n‚úÖ Accuracy: {accuracy:.2%} ({correct}/{total})\")\n",
    "    return accuracy\n",
    "\n",
    "# Evaluate\n",
    "accuracy = evaluate_reasoning(student_model, student_tokenizer, test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94079c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics\n",
    "results = {\n",
    "    'accuracy': float(accuracy),\n",
    "    'config': {\n",
    "        'teacher': config.TEACHER_MODEL,\n",
    "        'student': config.STUDENT_MODEL,\n",
    "        'lora_r': config.LORA_R,\n",
    "        'alpha_output': config.ALPHA_OUTPUT,\n",
    "        'beta_latent': config.BETA_LATENT\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{config.OUTPUT_DIR}/results.json\", 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"üìÅ Results saved!\")\n",
    "\n",
    "# Inference example\n",
    "def inference(question: str):\n",
    "    \"\"\"Single inference\"\"\"\n",
    "    prompt = prepare_prompt(question)\n",
    "    inputs = student_tokenizer(prompt, return_tensors='pt').to(student_model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = student_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=student_tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    result = student_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return result[len(prompt):]\n",
    "\n",
    "# Test inference\n",
    "test_question = \"If John has 5 apples and gives 2 to Mary, how many does he have left?\"\n",
    "print(f\"\\nüß™ Test Inference:\")\n",
    "print(f\"Q: {test_question}\")\n",
    "print(f\"A: {inference(test_question)}\")\n",
    "\n",
    "print(\"\\n‚ú® Pipeline complete! Model saved at:\", config.OUTPUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
