{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ae8a234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Installing packages...\n",
      "‚úÖ Installation complete!\n",
      "\n",
      "‚úÖ Installation complete!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# üì¶ Install Dependencies\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "\n",
    "packages = [\n",
    "    \"torch\",\n",
    "    \"transformers==4.36.0\",\n",
    "    \"peft==0.7.1\",\n",
    "    \"datasets==2.16.0\",\n",
    "    \"accelerate==0.25.0\",\n",
    "    \"bitsandbytes==0.41.3\",\n",
    "    \"tokenizers\",\n",
    "    \"tqdm\",\n",
    "    \"scikit-learn\"\n",
    "]\n",
    "\n",
    "print(\"üì¶ Installing packages...\")\n",
    "for pkg in packages:\n",
    "    try:\n",
    "        install_package(pkg)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Warning: Failed to install {pkg}: {e}\")\n",
    "\n",
    "print(\"‚úÖ Installation complete!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d03e4c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üñ•Ô∏è  SYSTEM INFORMATION\n",
      "======================================================================\n",
      "PyTorch: 2.7.1+cu118\n",
      "CUDA Available: True\n",
      "GPU: NVIDIA RTX A1000 Laptop GPU\n",
      "VRAM: 4.3 GB\n",
      "CUDA Version: 11.8\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# üîç System Check\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "HUGGING_FACE_TOKEN = os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üñ•Ô∏è  SYSTEM INFORMATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"‚ùå No GPU detected! This script requires CUDA.\")\n",
    "    raise RuntimeError(\"GPU is required for this training script\")\n",
    "\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d003547",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "E:\\Lib\\site-packages\\transformers\\utils\\generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "W1206 14:31:47.301000 4644 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "W1206 14:31:47.301000 4644 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "E:\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "E:\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "E:\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "E:\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ HuggingFace authenticated\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ‚öôÔ∏è Configuration\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from datasets import load_dataset\n",
    "import gc\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Disable unnecessary warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Training configuration\"\"\"\n",
    "    # Models\n",
    "    TEACHER_MODEL = \"meta-llama/Llama-2-13b-hf\"\n",
    "    STUDENT_MODEL = \"mistralai/Mistral-7B-v0.1\"\n",
    "    \n",
    "    # HuggingFace Token (replace with yours)\n",
    "    HF_TOKEN = HUGGING_FACE_TOKEN\n",
    "    \n",
    "    # Dataset\n",
    "    DATASET_NAME = \"gsm8k\"\n",
    "    DATASET_CONFIG = \"main\"\n",
    "    MAX_SAMPLES = 2000\n",
    "    MAX_LENGTH = 512\n",
    "    \n",
    "    # Training\n",
    "    BATCH_SIZE = 2\n",
    "    GRADIENT_ACCUM = 8\n",
    "    LEARNING_RATE = 2e-4\n",
    "    NUM_EPOCHS = 3\n",
    "    WARMUP_STEPS = 100\n",
    "    \n",
    "    # Distillation\n",
    "    ALPHA_OUTPUT = 0.5\n",
    "    BETA_LATENT = 0.5\n",
    "    TEMPERATURE = 2.0\n",
    "    LATENT_LAYERS = [8, 16, 24]  # Match these layers\n",
    "    \n",
    "    # LoRA\n",
    "    LORA_R = 16\n",
    "    LORA_ALPHA = 32\n",
    "    LORA_DROPOUT = 0.05\n",
    "    LORA_TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "    \n",
    "    # Local paths\n",
    "    OUTPUT_DIR = \"./distill_output\"\n",
    "    LATENT_CACHE_DIR = \"./latent_cache\"\n",
    "    \n",
    "    DEVICE = \"cuda\"\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(config.LATENT_CACHE_DIR, exist_ok=True)\n",
    "\n",
    "# Login to HuggingFace\n",
    "try:\n",
    "    login(token=config.HF_TOKEN, add_to_git_credential=False)\n",
    "    print(\"‚úÖ HuggingFace authenticated\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  HF login warning: {e}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234cb5ef",
   "metadata": {},
   "source": [
    "## üì• Download Models for Offline Use\n",
    "\n",
    "If you want to use models offline, run this cell **once** to download them:\n",
    "\n",
    "```python\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Download Teacher Model (Llama-2-13B) - ~26GB\n",
    "snapshot_download(\n",
    "    repo_id=\"meta-llama/Llama-2-13b-hf\",\n",
    "    local_dir=\"./models/Llama-2-13b-hf\",\n",
    "    token=\"YOUR_HF_TOKEN\"  # Required for Llama-2\n",
    ")\n",
    "\n",
    "# Download Student Model (Mistral-7B) - ~14GB\n",
    "snapshot_download(\n",
    "    repo_id=\"mistralai/Mistral-7B-v0.1\",\n",
    "    local_dir=\"./models/Mistral-7B-v0.1\"\n",
    ")\n",
    "```\n",
    "\n",
    "After downloading:\n",
    "1. Set `USE_LOCAL_MODELS = True` in Config cell\n",
    "2. Models will load from `./models/` directory\n",
    "\n",
    "**Total disk space needed: ~40GB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f2aa64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 19 files:  21%|‚ñà‚ñà        | 4/19 [00:03<00:11,  1.36it/s]"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=\"meta-llama/Llama-2-13b-hf\",\n",
    "    local_dir=\"./models/Llama-2-13b-hf\",\n",
    "    token=HUGGING_FACE_TOKEN,\n",
    "    resume_download=True\n",
    ")\n",
    "print(\"‚úÖ Teacher downloaded!\\n\")\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=\"mistralai/Mistral-7B-v0.1\",\n",
    "    local_dir=\"./models/Mistral-7B-v0.1\",\n",
    "    resume_download=True\n",
    ")\n",
    "print(\"‚úÖ Student downloaded!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdd0a666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Loading GSM8K dataset...\n",
      "‚úÖ Train samples: 2000\n",
      "‚úÖ Test samples: 500\n",
      "\n",
      "üìã Data sample:\n",
      "   Question: Natalia sold clips to 48 of her friends in April, and then she sold half as many...\n",
      "   Answer: Natalia sold 48/2 = <<48/2=24>>24 clips in May.\n",
      "Natalia sold 48+24 = <<48+24=72>...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# üìä Load Dataset\n",
    "print(\"üì¶ Loading GSM8K dataset...\")\n",
    "\n",
    "dataset = load_dataset(config.DATASET_NAME, config.DATASET_CONFIG, trust_remote_code=True)\n",
    "\n",
    "train_data = dataset['train'].select(range(min(config.MAX_SAMPLES, len(dataset['train']))))\n",
    "test_data = dataset['test'].select(range(min(500, len(dataset['test']))))\n",
    "\n",
    "print(f\"‚úÖ Train samples: {len(train_data)}\")\n",
    "print(f\"‚úÖ Test samples: {len(test_data)}\\n\")\n",
    "\n",
    "# Verify data structure\n",
    "sample = train_data[0]\n",
    "print(\"üìã Data sample:\")\n",
    "print(f\"   Question: {sample['question'][:80]}...\")\n",
    "print(f\"   Answer: {sample['answer'][:80]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aeaad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading Teacher (Llama-2-13B, 4-bit)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# üß† Teacher Model Loading & Extraction\n",
    "\n",
    "def prepare_prompt(question: str, answer: str = None) -> str:\n",
    "    \"\"\"Format prompt for math reasoning\"\"\"\n",
    "    prompt = f\"Question: {question}\\n\\nLet's solve this step by step:\\n\"\n",
    "    if answer:\n",
    "        prompt += answer\n",
    "    return prompt\n",
    "\n",
    "def check_cache_exists(cache_dir, expected_count):\n",
    "    \"\"\"Check if cache is complete\"\"\"\n",
    "    if not os.path.exists(cache_dir):\n",
    "        return 0\n",
    "    files = [f for f in os.listdir(cache_dir) if f.startswith('latent_') and f.endswith('.pt')]\n",
    "    return len(files)\n",
    "\n",
    "def load_teacher_model():\n",
    "    \"\"\"Load teacher with 4-bit quantization\"\"\"\n",
    "    print(\"üîÑ Loading Teacher (Llama-2-13B, 4-bit)...\")\n",
    "    \n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        config.TEACHER_MODEL,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        use_auth_token=True\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        config.TEACHER_MODEL,\n",
    "        use_auth_token=True\n",
    "    )\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    print(f\"‚úÖ Teacher loaded ({model.num_parameters() / 1e9:.1f}B params)\\n\")\n",
    "    return model, tokenizer\n",
    "\n",
    "def extract_latent_states(model, tokenizer, data, output_dir):\n",
    "    \"\"\"Extract and cache teacher latents\"\"\"\n",
    "    print(f\"üß† Extracting latents to: {output_dir}\")\n",
    "    \n",
    "    # Check cache\n",
    "    existing = check_cache_exists(output_dir, len(data))\n",
    "    if existing == len(data):\n",
    "        print(f\"‚úÖ Cache complete ({existing} files)\\n\")\n",
    "        return\n",
    "    \n",
    "    print(f\"   Found: {existing}/{len(data)} cached\")\n",
    "    print(f\"   Extracting: {len(data) - existing} samples\\n\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx in tqdm(range(len(data)), desc=\"Extracting\", ncols=80):\n",
    "            cache_path = os.path.join(output_dir, f\"latent_{idx}.pt\")\n",
    "            \n",
    "            if os.path.exists(cache_path):\n",
    "                continue\n",
    "            \n",
    "            item = data[idx]\n",
    "            prompt = prepare_prompt(item['question'], item.get('answer'))\n",
    "            \n",
    "            inputs = tokenizer(\n",
    "                prompt,\n",
    "                return_tensors='pt',\n",
    "                truncation=True,\n",
    "                max_length=config.MAX_LENGTH,\n",
    "                padding='max_length'\n",
    "            ).to(model.device)\n",
    "            \n",
    "            outputs = model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "            \n",
    "            # Extract specified layers\n",
    "            latent_states = {}\n",
    "            for layer_idx in config.LATENT_LAYERS:\n",
    "                if layer_idx < len(outputs.hidden_states):\n",
    "                    hidden = outputs.hidden_states[layer_idx]\n",
    "                    pooled = hidden.mean(dim=1).cpu()\n",
    "                    latent_states[f'layer_{layer_idx}'] = pooled\n",
    "            \n",
    "            torch.save(latent_states, cache_path)\n",
    "            \n",
    "            if idx % 100 == 0 and idx > 0:\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"‚úÖ Extraction complete!\\n\")\n",
    "\n",
    "# Execute extraction\n",
    "existing_cache = check_cache_exists(config.LATENT_CACHE_DIR, len(train_data))\n",
    "\n",
    "if existing_cache < len(train_data):\n",
    "    teacher_model, teacher_tokenizer = load_teacher_model()\n",
    "    extract_latent_states(teacher_model, teacher_tokenizer, train_data, config.LATENT_CACHE_DIR)\n",
    "    \n",
    "    # Free memory\n",
    "    del teacher_model, teacher_tokenizer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"üóëÔ∏è  Teacher freed from memory\\n\")\n",
    "else:\n",
    "    print(f\"‚úÖ Cache found ({existing_cache} files)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646452c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéì Loading Student (Mistral-7B + LoRA)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# üéì Student Model Setup\n",
    "\n",
    "class ReasoningDataset(Dataset):\n",
    "    \"\"\"Dataset with cached teacher latents\"\"\"\n",
    "    def __init__(self, data, tokenizer, max_length=512, latent_dir=None):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.latent_dir = latent_dir\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        prompt = prepare_prompt(item['question'], item.get('answer'))\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        result = {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'idx': idx\n",
    "        }\n",
    "        \n",
    "        # Load cached latents\n",
    "        if self.latent_dir:\n",
    "            latent_path = os.path.join(self.latent_dir, f\"latent_{idx}.pt\")\n",
    "            if os.path.exists(latent_path):\n",
    "                result['teacher_latents'] = torch.load(latent_path, map_location='cpu')\n",
    "        \n",
    "        return result\n",
    "\n",
    "def setup_student_model():\n",
    "    \"\"\"Load student with LoRA\"\"\"\n",
    "    print(\"üéì Loading Student (Mistral-7B + LoRA)...\")\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        config.STUDENT_MODEL,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.STUDENT_MODEL)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Apply LoRA\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        r=config.LORA_R,\n",
    "        lora_alpha=config.LORA_ALPHA,\n",
    "        lora_dropout=config.LORA_DROPOUT,\n",
    "        target_modules=config.LORA_TARGET_MODULES,\n",
    "        bias=\"none\"\n",
    "    )\n",
    "    \n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    print(f\"‚úÖ Student loaded\")\n",
    "    print(f\"   Total params: {total_params / 1e6:.1f}M\")\n",
    "    print(f\"   Trainable: {trainable_params / 1e6:.1f}M ({100 * trainable_params / total_params:.2f}%)\\n\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "student_model, student_tokenizer = setup_student_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edf27d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üî• Training Setup\n",
    "\n",
    "class DistillationTrainer(Trainer):\n",
    "    \"\"\"Custom trainer with latent distillation\"\"\"\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        outputs = model(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            labels=inputs['input_ids'],\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        loss_output = outputs.loss\n",
    "        loss_latent = 0.0\n",
    "        \n",
    "        # Latent matching\n",
    "        if 'teacher_latents' in inputs:\n",
    "            teacher_latents = inputs['teacher_latents']\n",
    "            student_hidden = outputs.hidden_states\n",
    "            \n",
    "            matched_layers = 0\n",
    "            for layer_idx in config.LATENT_LAYERS:\n",
    "                layer_key = f'layer_{layer_idx}'\n",
    "                if layer_key in teacher_latents and layer_idx < len(student_hidden):\n",
    "                    student_h = student_hidden[layer_idx].mean(dim=1)\n",
    "                    teacher_h = teacher_latents[layer_key].to(student_h.device)\n",
    "                    \n",
    "                    loss_latent += F.mse_loss(student_h, teacher_h)\n",
    "                    matched_layers += 1\n",
    "            \n",
    "            if matched_layers > 0:\n",
    "                loss_latent /= matched_layers\n",
    "        \n",
    "        total_loss = config.ALPHA_OUTPUT * loss_output + config.BETA_LATENT * loss_latent\n",
    "        \n",
    "        return (total_loss, outputs) if return_outputs else total_loss\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset = ReasoningDataset(\n",
    "    train_data,\n",
    "    student_tokenizer,\n",
    "    max_length=config.MAX_LENGTH,\n",
    "    latent_dir=config.LATENT_CACHE_DIR\n",
    ")\n",
    "\n",
    "test_dataset = ReasoningDataset(\n",
    "    test_data,\n",
    "    student_tokenizer,\n",
    "    max_length=config.MAX_LENGTH,\n",
    "    latent_dir=None\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=config.OUTPUT_DIR,\n",
    "    num_train_epochs=config.NUM_EPOCHS,\n",
    "    per_device_train_batch_size=config.BATCH_SIZE,\n",
    "    per_device_eval_batch_size=config.BATCH_SIZE,\n",
    "    gradient_accumulation_steps=config.GRADIENT_ACCUM,\n",
    "    learning_rate=config.LEARNING_RATE,\n",
    "    warmup_steps=config.WARMUP_STEPS,\n",
    "    logging_steps=50,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "    logging_dir=f\"{config.OUTPUT_DIR}/logs\",\n",
    "    disable_tqdm=False\n",
    ")\n",
    "\n",
    "trainer = DistillationTrainer(\n",
    "    model=student_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training configured\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d61368e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ Start Training\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üî• STARTING TRAINING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Epochs: {config.NUM_EPOCHS}\")\n",
    "print(f\"Batch size: {config.BATCH_SIZE} √ó {config.GRADIENT_ACCUM} = {config.BATCH_SIZE * config.GRADIENT_ACCUM}\")\n",
    "print(f\"Learning rate: {config.LEARNING_RATE}\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save final model\n",
    "trainer.save_model(f\"{config.OUTPUT_DIR}/final_model\")\n",
    "student_tokenizer.save_pretrained(f\"{config.OUTPUT_DIR}/final_model\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ TRAINING COMPLETE\")\n",
    "print(\"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0408d719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Evaluation\n",
    "\n",
    "def evaluate_reasoning(model, tokenizer, test_data, num_samples=50):\n",
    "    \"\"\"Evaluate on reasoning tasks\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    \n",
    "    print(f\"üìä Evaluating on {num_samples} samples...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx in tqdm(range(min(num_samples, len(test_data))), desc=\"Evaluating\", ncols=80):\n",
    "            item = test_data[idx]\n",
    "            prompt = prepare_prompt(item['question'])\n",
    "            \n",
    "            inputs = tokenizer(\n",
    "                prompt,\n",
    "                return_tensors='pt',\n",
    "                truncation=True,\n",
    "                max_length=256\n",
    "            ).to(model.device)\n",
    "            \n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=128,\n",
    "                temperature=0.7,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            \n",
    "            generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            ground_truth = str(item['answer'])\n",
    "            \n",
    "            if ground_truth in generated:\n",
    "                correct += 1\n",
    "    \n",
    "    accuracy = correct / num_samples\n",
    "    print(f\"‚úÖ Accuracy: {accuracy:.2%} ({correct}/{num_samples})\\n\")\n",
    "    return accuracy\n",
    "\n",
    "accuracy = evaluate_reasoning(student_model, student_tokenizer, test_data)\n",
    "\n",
    "# Save results\n",
    "results = {\n",
    "    'accuracy': float(accuracy),\n",
    "    'config': {\n",
    "        'teacher': config.TEACHER_MODEL,\n",
    "        'student': config.STUDENT_MODEL,\n",
    "        'lora_r': config.LORA_R,\n",
    "        'alpha_output': config.ALPHA_OUTPUT,\n",
    "        'beta_latent': config.BETA_LATENT,\n",
    "        'epochs': config.NUM_EPOCHS\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{config.OUTPUT_DIR}/results.json\", 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"üìÅ Results saved to: {config.OUTPUT_DIR}/results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d421527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ Test Inference\n",
    "\n",
    "def inference(question: str):\n",
    "    \"\"\"Single question inference\"\"\"\n",
    "    prompt = prepare_prompt(question)\n",
    "    inputs = student_tokenizer(prompt, return_tensors='pt').to(student_model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = student_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=student_tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    result = student_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return result[len(prompt):]\n",
    "\n",
    "# Test examples\n",
    "test_questions = [\n",
    "    \"If John has 5 apples and gives 2 to Mary, how many does he have left?\",\n",
    "    \"A train travels 60 miles in 1 hour. How far will it travel in 3.5 hours?\",\n",
    "    \"Sarah has $50. She spends $15 on lunch and $12 on a book. How much money does she have left?\"\n",
    "]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üß™ INFERENCE EXAMPLES\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "for i, q in enumerate(test_questions, 1):\n",
    "    print(f\"Q{i}: {q}\")\n",
    "    answer = inference(q)\n",
    "    print(f\"A{i}: {answer}\\n\")\n",
    "    print(\"-\"*70 + \"\\n\")\n",
    "\n",
    "print(f\"‚úÖ Model saved to: {config.OUTPUT_DIR}/final_model\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
